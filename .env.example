# Zen MCP Server Environment Configuration
# Copy this file to .env and fill in your values

# Required: Workspace root directory for file access
# This should be the HOST path that contains all files Claude might reference
# Defaults to $HOME for direct usage, auto-configured for Docker
WORKSPACE_ROOT=/Users/your-username

# API Keys - At least one is required
#
# IMPORTANT: Use EITHER OpenRouter OR native APIs (Gemini/OpenAI), not both!
# Having both creates ambiguity about which provider serves each model.
#
# Option 1: Use native APIs (recommended for direct access)
# Get your Gemini API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Get your OpenAI API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Option 2: Use OpenRouter for access to multiple models through one API
# Get your OpenRouter API key from: https://openrouter.ai/
# If using OpenRouter, comment out the native API keys above
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Option 3: Use custom API endpoints for local models (Ollama, vLLM, LM Studio, etc.)
# IMPORTANT: Since this server ALWAYS runs in Docker, you MUST use host.docker.internal instead of localhost
# ❌ WRONG: http://localhost:11434/v1 (Docker containers cannot reach localhost)
# ✅ CORRECT: http://host.docker.internal:11434/v1 (Docker can reach host services)
CUSTOM_API_URL=http://host.docker.internal:11434/v1  # Ollama example (NOT localhost!)
CUSTOM_API_KEY=                                      # Empty for Ollama (no auth needed)
CUSTOM_MODEL_NAME=llama3.2                          # Default model name

# Optional: Default model to use
# Options: 'auto' (Claude picks best model), 'pro', 'flash', 'o3', 'o4-mini'
# When set to 'auto', Claude will select the best model for each task
# Defaults to 'auto' if not specified
DEFAULT_MODEL=auto

# Optional: Default thinking mode for ThinkDeep tool
# NOTE: Only applies to models that support extended thinking (e.g., Gemini 2.5 Pro)
#       Flash models (2.0) will use system prompt engineering instead
# Token consumption per mode:
#   minimal: 128 tokens   - Quick analysis, fastest response
#   low:     2,048 tokens - Light reasoning tasks  
#   medium:  8,192 tokens - Balanced reasoning (good for most cases)
#   high:    16,384 tokens - Complex analysis (recommended for thinkdeep)
#   max:     32,768 tokens - Maximum reasoning depth, slowest but most thorough
# Defaults to 'high' if not specified
DEFAULT_THINKING_MODE_THINKDEEP=high

# Optional: Custom model configuration file path
# Override the default location of custom_models.json
# CUSTOM_MODELS_CONFIG_PATH=/path/to/your/custom_models.json

# Optional: Redis configuration (auto-configured for Docker)
# The Redis URL for conversation threading - typically managed by docker-compose
# REDIS_URL=redis://redis:6379/0

# Optional: Logging level (DEBUG, INFO, WARNING, ERROR)
# DEBUG: Shows detailed operational messages for troubleshooting
# INFO: Shows general operational messages (default)
# WARNING: Shows only warnings and errors
# ERROR: Shows only errors
LOG_LEVEL=INFO

# Optional: Project root override for file sandboxing
# If set, overrides the default sandbox directory
# Use with caution - this controls which files the server can access
# MCP_PROJECT_ROOT=/path/to/specific/project